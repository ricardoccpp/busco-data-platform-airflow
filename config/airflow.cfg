[core]
# The folder where your airflow pipelines live, most likely a
# subfolder in a code repository. This path must be absolute.
dags_folder = /opt/airflow/dags

# The folder where airflow should store its log files
# This path must be absolute
base_log_folder = /opt/airflow/logs

# The executor class that airflow should use. Choices include
# SequentialExecutor, LocalExecutor, CeleryExecutor, DaskExecutor
executor = CeleryExecutor

# Default timezone in case supplied date times are naive
default_timezone = utc

# The home folder for airflow, default is ~/airflow
airflow_home = /opt/airflow

# Default configuration for dags
dags_are_paused_at_creation = True
load_examples = False
dag_discovery_safe_mode = True
dagbag_import_timeout = 60
fernet_key = ${AIRFLOW__CORE__FERNET_KEY}

# Whether to disable pickling dags
donot_pickle = False

# How long a DagRun should be up for if it has not been updated in the database
dagrun_timeout = 24h

# How often the DAG processor should search for new files
min_file_process_interval = 60
dag_file_processor_timeout = 50
max_active_tasks_per_dag = 16
max_active_runs_per_dag = 16
default_task_retries = 3
default_task_retry_delay = 5

# Access permission default roles
[webserver]
# The base url of your website as airflow sees it
base_url = http://localhost:8080

# The webserver host and port
web_server_host = 0.0.0.0
web_server_port = 8080

# Secret key used to run your flask app
# It should be as random as possible
secret_key = ${AIRFLOW__WEBSERVER__SECRET_KEY:=temporary_key}

# Number of seconds the webserver waits on a process response
web_server_worker_timeout = 120

# Number of workers to refresh the webserver
workers = 4

# Time for web server workers to refresh
worker_refresh_batch_size = 1
worker_refresh_interval = 30

# Use the service name when generating links
enable_proxy_fix = True

# Default number of task instances displayed on UI
dag_default_view = graph
log_fetch_timeout_sec = 5

# Number of initial access items displayed in UI
navbar_color = #fff
default_dag_run_display_number = 25

# Default search order
default_ui_timezone = UTC
dag_orientation = LR

# Enable authentication
authenticate = True
auth_backend = airflow.api.auth.backend.session,airflow.api.auth.backend.basic_auth

# UI customization
hide_paused_dags_by_default = False
page_size = 100

[api]
# Exposes the ability to fetch DAG runs and tasks through the API
auth_backend = airflow.api.auth.backend.basic_auth,airflow.api.auth.backend.session

[smtp]
# If you want airflow to send emails on retries, failure, etc,
# use this section

# SMTP server
smtp_host = smtp.gmail.com
smtp_starttls = True
smtp_ssl = False
smtp_port = 587
smtp_mail_from = ${AIRFLOW__SMTP__SMTP_MAIL_FROM:=airflow@example.com}
smtp_user = ${AIRFLOW__SMTP__SMTP_USER:=}
smtp_password = ${AIRFLOW__SMTP__SMTP_PASSWORD:=}

[celery]
# This section only applies if you are using the CeleryExecutor
# in core configuration

# The app name that will be used by celery
celery_app_name = airflow.executors.celery_executor

# The concurrency that will be used when starting workers with the
# "airflow celery worker" command
worker_concurrency = 16

# The broker URL that should be used to connect to the redis instance
broker_url = ${AIRFLOW__CELERY__BROKER_URL}

# Result backend to track task states
result_backend = ${AIRFLOW__CELERY__RESULT_BACKEND}

# Celery worker startup command
worker_autoscale = 16,8

# Default queue to be used for tasks
default_queue = default

# Import path for celery configuration options
celery_config_options = airflow.config_templates.default_celery.DEFAULT_CELERY_CONFIG

[scheduler]
# Task instances listen for external kill signal (when you clear tasks
# from the CLI or the UI), this defines the frequency at which they should
# listen (in seconds).
job_heartbeat_sec = 5

# The scheduler constantly tries to trigger new tasks (look at the
# scheduler section in the docs for more information).
# This defines how often the scheduler should run (in seconds).
run_duration = -1
min_file_process_interval = 30
scheduler_heartbeat_sec = 5

# The scheduler can run multiple threads in parallel to schedule dags.
# This defines how many threads will run.
max_threads = 2
catchup_by_default = False

# How many DagRuns should we create at a time
max_dagruns_to_create_per_loop = 10

# Statsd (metrics) settings
statsd_on = False
statsd_host = localhost
statsd_port = 8125
statsd_prefix = airflow

# How many seconds to wait between file-parsing loops to prevent the logs from being spammed.
min_file_parsing_loop_time = 1

# How many seconds to wait between checking for tasks to create
scheduler_idle_sleep_time = 1

[logging]
# The folder where airflow should store its log files.
# This path must be absolute
base_log_folder = /opt/airflow/logs

# Logging level
logging_level = INFO

# Log format
log_format = [%%(asctime)s] {%%(filename)s:%%(lineno)d} %%(levelname)s - %%(message)s
simple_log_format = %%(asctime)s %%(levelname)s - %%(message)s

# Log filename format
log_filename_template = {{ ti.dag_id }}/{{ ti.task_id }}/{{ ts }}/{{ try_number }}.log
log_processor_filename_template = {{ filename }}.log

# Logging configuration
task_log_reader = task

# How many rotated log files to keep
log_rotation_count = 30

[secrets]
# Full class path of the secrets backend
backend = 

[kubernetes]
# Kubernetes configuration (if using KubernetesExecutor)
# This section only applies if you are using the KubernetesExecutor in core configuration
worker_container_repository = apache/airflow
worker_container_tag = 2.8.0-python3.10
namespace = default
delete_worker_pods = True
run_as_user = 50000
fs_group = 0
worker_pods_creation_batch_size = 16
cluster_context = 
config_file = 
kube_client_request_args = {}
delete_option_kwargs = {}
multi_namespace_mode = False
multi_namespace_mode_namespace_list = []
worker_service_account_name = default

[elasticsearch]
# Elasticsearch configuration (for logging with Elasticsearch)
log_id_template = {dag_id}-{task_id}-{execution_date}-{try_number}
end_of_log_mark = end_of_log
frontend = localhost:5601
write_stdout = False
json_fields = asctime, filename, lineno, levelname, message

[elasticsearch_configs]
# Additional Elasticsearch configuration
use_ssl = False
verify_certs = True